2025-10-27 16:35:05 - Starting default daily short-term run
2025-10-27 16:35:05 - Running Spark job...
25/10/27 16:35:06 INFO spark.SparkContext: Running Spark version 2.4.0-cdh6.3.3
25/10/27 16:35:06 INFO logging.DriverLogger: Added a local log appender at: /tmp/spark-f67bb3db-401b-4395-9d1c-23fc002782d2/__driver_logs__/driver.log
25/10/27 16:35:06 INFO spark.SparkContext: Submitted application: Online_Model_Short_Term_Daily
25/10/27 16:35:06 INFO spark.SecurityManager: Changing view acls to: cadmin
25/10/27 16:35:06 INFO spark.SecurityManager: Changing modify acls to: cadmin
25/10/27 16:35:06 INFO spark.SecurityManager: Changing view acls groups to: 
25/10/27 16:35:06 INFO spark.SecurityManager: Changing modify acls groups to: 
25/10/27 16:35:06 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cadmin); groups with view permissions: Set(); users  with modify permissions: Set(cadmin); groups with modify permissions: Set()
25/10/27 16:35:06 INFO util.Utils: Successfully started service 'sparkDriver' on port 41012.
25/10/27 16:35:06 INFO spark.SparkEnv: Registering MapOutputTracker
25/10/27 16:35:06 INFO spark.SparkEnv: Registering BlockManagerMaster
25/10/27 16:35:06 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/10/27 16:35:06 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/10/27 16:35:06 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-cc83c615-e312-4115-b521-703d5c78c93d
25/10/27 16:35:07 INFO memory.MemoryStore: MemoryStore started with capacity 3.0 GB
25/10/27 16:35:07 INFO spark.SparkEnv: Registering OutputCommitCoordinator
25/10/27 16:35:07 INFO util.log: Logging initialized @1754ms
25/10/27 16:35:07 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-09-04T23:11:46+02:00, git hash: 3ce520221d0240229c862b122d2b06c12a625732
25/10/27 16:35:07 INFO server.Server: Started @1821ms
25/10/27 16:35:07 INFO server.AbstractConnector: Started ServerConnector@8892de1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
25/10/27 16:35:07 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@e368659{/jobs,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30295751{/jobs/json,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9b891b9{/jobs/job,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5bf18db5{/jobs/job/json,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2198ca45{/stages,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c60b296{/stages/json,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@56f9d0dc{/stages/stage,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f82b155{/stages/stage/json,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1765ef49{/stages/pool,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d9b8cd{/stages/pool/json,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1902343f{/storage,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3902653{/storage/json,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3bb78ecf{/storage/rdd,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@736056a4{/storage/rdd/json,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7bcb373b{/environment,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@796b24af{/environment/json,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7ead1cbb{/executors,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f09b0bc{/executors/json,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d592c89{/executors/threadDump,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d373bd1{/executors/threadDump/json,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b69b608{/static,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23765db3{/,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@80f70af{/api,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53cf367c{/jobs/job/kill,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@711cd67e{/stages/stage/kill,null,AVAILABLE,@Spark}
25/10/27 16:35:07 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://lakenode3.datalake01.eg:4040
25/10/27 16:35:07 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 16:35:07 INFO Configuration.deprecation: No unit for dfs.client.datanode-restart.timeout(30) assuming SECONDS
25/10/27 16:35:07 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm278
25/10/27 16:35:07 INFO yarn.Client: Requesting a new application from cluster with 6 NodeManagers
25/10/27 16:35:07 INFO conf.Configuration: resource-types.xml not found
25/10/27 16:35:07 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
25/10/27 16:35:08 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (65536 MB per container)
25/10/27 16:35:08 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
25/10/27 16:35:08 INFO yarn.Client: Setting up container launch context for our AM
25/10/27 16:35:08 INFO yarn.Client: Setting up the launch environment for our AM container
25/10/27 16:35:08 INFO yarn.Client: Preparing resources for our AM container
25/10/27 16:35:08 INFO yarn.Client: Uploading resource file:/tmp/spark-f67bb3db-401b-4395-9d1c-23fc002782d2/__spark_conf__1656077325971719088.zip -> hdfs://Isilon-ET-hadoop-cdh.datalake01.eg:8020/user/cadmin/.sparkStaging/application_1756032282212_36951/__spark_conf__.zip
25/10/27 16:35:08 INFO Configuration.deprecation: No unit for dfs.client.datanode-restart.timeout(30) assuming SECONDS
25/10/27 16:35:08 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 16:35:08 INFO spark.SecurityManager: Changing view acls to: cadmin
25/10/27 16:35:08 INFO spark.SecurityManager: Changing modify acls to: cadmin
25/10/27 16:35:08 INFO spark.SecurityManager: Changing view acls groups to: 
25/10/27 16:35:08 INFO spark.SecurityManager: Changing modify acls groups to: 
25/10/27 16:35:08 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cadmin); groups with view permissions: Set(); users  with modify permissions: Set(cadmin); groups with modify permissions: Set()
25/10/27 16:35:08 INFO yarn.Client: Submitting application application_1756032282212_36951 to ResourceManager
25/10/27 16:35:08 INFO impl.YarnClientImpl: Submitted application application_1756032282212_36951
25/10/27 16:35:09 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 16:35:09 INFO yarn.Client: Application report for application_1756032282212_36951 (state: ACCEPTED)
25/10/27 16:35:09 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: root.users.cadmin
	 start time: 1761572108341
	 final status: UNDEFINED
	 tracking URL: http://lakemaster3.datalake01.eg:8088/proxy/application_1756032282212_36951/
	 user: cadmin
25/10/27 16:35:10 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 16:35:10 INFO yarn.Client: Application report for application_1756032282212_36951 (state: RUNNING)
25/10/27 16:35:10 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 10.208.244.22
	 ApplicationMaster RPC port: -1
	 queue: root.users.cadmin
	 start time: 1761572108341
	 final status: UNDEFINED
	 tracking URL: http://lakemaster3.datalake01.eg:8088/proxy/application_1756032282212_36951/
	 user: cadmin
25/10/27 16:35:10 INFO cluster.YarnClientSchedulerBackend: Application application_1756032282212_36951 has started running.
25/10/27 16:35:10 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1756032282212_36951 and attemptId None
25/10/27 16:35:10 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44086.
25/10/27 16:35:10 INFO netty.NettyBlockTransferService: Server created on lakenode3.datalake01.eg:44086
25/10/27 16:35:10 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/10/27 16:35:10 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, lakenode3.datalake01.eg, 44086, None)
25/10/27 16:35:10 INFO storage.BlockManagerMasterEndpoint: Registering block manager lakenode3.datalake01.eg:44086 with 3.0 GB RAM, BlockManagerId(driver, lakenode3.datalake01.eg, 44086, None)
25/10/27 16:35:10 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, lakenode3.datalake01.eg, 44086, None)
25/10/27 16:35:10 INFO storage.BlockManager: external shuffle service port = 7337
25/10/27 16:35:10 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, lakenode3.datalake01.eg, 44086, None)
25/10/27 16:35:10 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> lakemaster2.datalake01.eg,lakemaster3.datalake01.eg, PROXY_URI_BASES -> http://lakemaster2.datalake01.eg:8088/proxy/application_1756032282212_36951,http://lakemaster3.datalake01.eg:8088/proxy/application_1756032282212_36951, RM_HA_URLS -> lakemaster2.datalake01.eg:8088,lakemaster3.datalake01.eg:8088), /proxy/application_1756032282212_36951
25/10/27 16:35:10 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.
25/10/27 16:35:10 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.
25/10/27 16:35:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b4168ff{/metrics/json,null,AVAILABLE,@Spark}
25/10/27 16:35:10 INFO scheduler.EventLoggingListener: Logging events to hdfs://Isilon-ET-hadoop-cdh.datalake01.eg:8020/user/spark/applicationHistory/application_1756032282212_36951
25/10/27 16:35:10 INFO spark.SparkContext: Registered listener com.cloudera.spark.lineage.NavigatorAppListener
25/10/27 16:35:10 INFO logging.DriverLogger$DfsAsyncWriter: Started driver log file sync to: /user/spark/driverLogs/application_1756032282212_36951_driver.log
25/10/27 16:35:11 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
25/10/27 16:35:11 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 16:35:12 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 16:35:13 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 16:35:13 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.208.244.22:58988) with ID 2
25/10/27 16:35:13 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.208.244.22:58990) with ID 3
25/10/27 16:35:13 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.208.244.22:58992) with ID 1
25/10/27 16:35:13 INFO storage.BlockManagerMasterEndpoint: Registering block manager lakemaster2.datalake01.eg:33643 with 5.2 GB RAM, BlockManagerId(3, lakemaster2.datalake01.eg, 33643, None)
25/10/27 16:35:13 INFO storage.BlockManagerMasterEndpoint: Registering block manager lakemaster2.datalake01.eg:39138 with 5.2 GB RAM, BlockManagerId(1, lakemaster2.datalake01.eg, 39138, None)
25/10/27 16:35:13 INFO storage.BlockManagerMasterEndpoint: Registering block manager lakemaster2.datalake01.eg:40931 with 5.2 GB RAM, BlockManagerId(2, lakemaster2.datalake01.eg, 40931, None)
25/10/27 16:35:14 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.208.244.27:57622) with ID 5
25/10/27 16:35:14 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.208.244.27:57624) with ID 4
25/10/27 16:35:14 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.208.244.27:57626) with ID 6
25/10/27 16:35:14 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
25/10/27 16:35:14 INFO storage.BlockManagerMasterEndpoint: Registering block manager lakenode4.datalake01.eg:43797 with 5.2 GB RAM, BlockManagerId(5, lakenode4.datalake01.eg, 43797, None)
25/10/27 16:35:14 INFO storage.BlockManagerMasterEndpoint: Registering block manager lakenode4.datalake01.eg:36610 with 5.2 GB RAM, BlockManagerId(6, lakenode4.datalake01.eg, 36610, None)
25/10/27 16:35:14 INFO storage.BlockManagerMasterEndpoint: Registering block manager lakenode4.datalake01.eg:34609 with 5.2 GB RAM, BlockManagerId(4, lakenode4.datalake01.eg, 34609, None)
25/10/27 16:35:14 INFO internal.SharedState: loading hive config file: file:/etc/hive/conf.cloudera.hive/hive-site.xml
25/10/27 16:35:14 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('/user/hive/warehouse') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
25/10/27 16:35:14 INFO internal.SharedState: Warehouse path is '/user/hive/warehouse'.
25/10/27 16:35:14 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.
25/10/27 16:35:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33c5089{/SQL,null,AVAILABLE,@Spark}
25/10/27 16:35:14 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.
25/10/27 16:35:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c868e99{/SQL/json,null,AVAILABLE,@Spark}
25/10/27 16:35:14 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.
25/10/27 16:35:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c4733b2{/SQL/execution,null,AVAILABLE,@Spark}
25/10/27 16:35:14 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.
25/10/27 16:35:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c6bcd44{/SQL/execution/json,null,AVAILABLE,@Spark}
25/10/27 16:35:14 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.
25/10/27 16:35:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2dae947c{/static/sql,null,AVAILABLE,@Spark}
25/10/27 16:35:14 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
2025-10-27 16:35:14 INFO - Starting default short-term run for 2025-10-25 to 2025-10-27
2025-10-27 16:35:14 INFO - Executing base query for short-term run (3-day window)...
25/10/27 16:35:16 INFO conf.HiveConf: Found configuration file file:/etc/hive/conf.cloudera.hive/hive-site.xml
25/10/27 16:35:16 INFO hive.HiveUtils: Initializing HiveMetastoreConnection version 2.1 using Spark classes.
25/10/27 16:35:16 INFO conf.HiveConf: Found configuration file file:/etc/hive/conf.cloudera.hive/hive-site.xml
25/10/27 16:35:16 INFO session.SessionState: Created HDFS directory: /tmp/hive/cadmin/c8784e26-de90-4e46-96d2-ca70f24b334f
25/10/27 16:35:16 INFO session.SessionState: Created local directory: /tmp/cadmin/c8784e26-de90-4e46-96d2-ca70f24b334f
25/10/27 16:35:16 INFO session.SessionState: Created HDFS directory: /tmp/hive/cadmin/c8784e26-de90-4e46-96d2-ca70f24b334f/_tmp_space.db
25/10/27 16:35:16 INFO client.HiveClientImpl: Warehouse location for Hive client (version 2.1.1) is /user/hive/warehouse
25/10/27 16:35:16 INFO hive.metastore: HMS client filtering is enabled.
25/10/27 16:35:16 INFO hive.metastore: Trying to connect to metastore with URI thrift://lakemaster1.datalake01.eg:9083
25/10/27 16:35:16 INFO hive.metastore: Opened a connection to metastore, current connections: 1
25/10/27 16:35:17 INFO hive.metastore: Connected to metastore.
25/10/27 16:35:17 INFO metadata.Hive: Registering function fix_arabic FixArabicUDF
2025-10-27 16:35:17 INFO - Computing category-level aggregates...
2025-10-27 16:35:17 INFO - Computing KPIs (no report will be generated or printed)...
2025-10-27 16:35:18 INFO - Writing KPI output into analytics_prod.oim_short_term_raw partition dt=2025-10-27 (overwrite)...
25/10/27 16:35:18 WARN util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
2025-10-27 16:35:18 ERROR - Job failed: u'`analytics_prod`.`oim_short_term_raw` requires that the data to be inserted have the same number of columns as the target table: target table has 9 column(s) but the inserted data has 58 column(s), including 1 partition column(s) having constant value(s).;'
25/10/27 16:35:18 INFO server.AbstractConnector: Stopped Spark@8892de1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
25/10/27 16:35:18 INFO ui.SparkUI: Stopped Spark web UI at http://lakenode3.datalake01.eg:4040
25/10/27 16:35:18 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread
25/10/27 16:35:18 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors
25/10/27 16:35:18 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
25/10/27 16:35:18 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
25/10/27 16:35:18 INFO cluster.YarnClientSchedulerBackend: Stopped
25/10/27 16:35:18 WARN netty.Dispatcher: Message RemoteProcessDisconnected(10.208.244.27:57622) dropped. Could not find HeartbeatReceiver.
25/10/27 16:35:18 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/10/27 16:35:18 INFO memory.MemoryStore: MemoryStore cleared
25/10/27 16:35:18 INFO storage.BlockManager: BlockManager stopped
25/10/27 16:35:18 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
25/10/27 16:35:18 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/10/27 16:35:18 INFO spark.SparkContext: Successfully stopped SparkContext
Traceback (most recent call last):
  File "/home/cadmin/Scripts/Analytics_Scripts/oim_short_term/combined_query_daily.py", line 266, in <module>
    main()
  File "/home/cadmin/Scripts/Analytics_Scripts/oim_short_term/combined_query_daily.py", line 226, in main
    spark.sql("INSERT OVERWRITE TABLE {0} PARTITION (dt = '{1}') SELECT * FROM tmp_kpi".format(target_table, st_to_date))
  File "/opt/cloudera/parcels/CDH-6.3.3-1.cdh6.3.3.p0.1796617/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 778, in sql
  File "/opt/cloudera/parcels/CDH-6.3.3-1.cdh6.3.3.p0.1796617/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/opt/cloudera/parcels/CDH-6.3.3-1.cdh6.3.3.p0.1796617/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 69, in deco
pyspark.sql.utils.AnalysisException: u'`analytics_prod`.`oim_short_term_raw` requires that the data to be inserted have the same number of columns as the target table: target table has 9 column(s) but the inserted data has 58 column(s), including 1 partition column(s) having constant value(s).;'
25/10/27 16:35:19 INFO util.ShutdownHookManager: Shutdown hook called
25/10/27 16:35:19 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-b3ba8103-6e86-42fe-b394-898e5e51fc4e
25/10/27 16:35:19 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f67bb3db-401b-4395-9d1c-23fc002782d2
25/10/27 16:35:19 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f67bb3db-401b-4395-9d1c-23fc002782d2/pyspark-66a3226a-9ac7-4420-be61-a404e9e2d2cf
2025-10-27 16:35:19 - ERROR: Short-term job failed. Check logs for details.
