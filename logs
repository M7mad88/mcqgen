2025-10-27 11:12:24 - Starting default daily short-term run
25/10/27 11:12:25 INFO spark.SparkContext: Running Spark version 2.4.0-cdh6.3.3
25/10/27 11:12:25 INFO logging.DriverLogger: Added a local log appender at: /tmp/spark-4f41f3d4-aa6e-4779-9d3c-acc11ab6eabe/__driver_logs__/driver.log
25/10/27 11:12:25 INFO spark.SparkContext: Submitted application: Online_Model_Short_Term_Daily
25/10/27 11:12:25 INFO spark.SecurityManager: Changing view acls to: cadmin
25/10/27 11:12:25 INFO spark.SecurityManager: Changing modify acls to: cadmin
25/10/27 11:12:25 INFO spark.SecurityManager: Changing view acls groups to: 
25/10/27 11:12:25 INFO spark.SecurityManager: Changing modify acls groups to: 
25/10/27 11:12:25 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cadmin); groups with view permissions: Set(); users  with modify permissions: Set(cadmin); groups with modify permissions: Set()
25/10/27 11:12:25 INFO util.Utils: Successfully started service 'sparkDriver' on port 41799.
25/10/27 11:12:26 INFO spark.SparkEnv: Registering MapOutputTracker
25/10/27 11:12:26 INFO spark.SparkEnv: Registering BlockManagerMaster
25/10/27 11:12:26 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/10/27 11:12:26 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/10/27 11:12:26 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-49889755-5cb6-4d98-ba6f-808982281776
25/10/27 11:12:26 INFO memory.MemoryStore: MemoryStore started with capacity 4.1 GB
25/10/27 11:12:26 INFO spark.SparkEnv: Registering OutputCommitCoordinator
25/10/27 11:12:26 INFO util.log: Logging initialized @1719ms
25/10/27 11:12:26 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-09-04T23:11:46+02:00, git hash: 3ce520221d0240229c862b122d2b06c12a625732
25/10/27 11:12:26 INFO server.Server: Started @1785ms
25/10/27 11:12:26 INFO server.AbstractConnector: Started ServerConnector@1e891e0d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
25/10/27 11:12:26 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@14b64abf{/jobs,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@705496{/jobs/json,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d3231fe{/jobs/job,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@77304dd9{/jobs/job/json,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7566634d{/stages,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9a711de{/stages/json,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53402459{/stages/stage,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@514952d3{/stages/stage/json,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f9c64f{/stages/pool,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a400bd0{/stages/pool/json,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@164ac4c8{/storage,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d0ec318{/storage/json,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4008b748{/storage/rdd,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3aec3a12{/storage/rdd/json,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a40d503{/environment,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@12ddf50{/environment/json,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41a8aca{/executors,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f7d1cf1{/executors/json,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@236932d0{/executors/threadDump,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4c51aeb2{/executors/threadDump/json,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ccd3128{/static,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71fffffd{/,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5585e3ad{/api,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36306e57{/jobs/job/kill,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c5f836b{/stages/stage/kill,null,AVAILABLE,@Spark}
25/10/27 11:12:26 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://lakenode3.datalake01.eg:4040
25/10/27 11:12:26 INFO util.Utils: Using initial executors = 10, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
25/10/27 11:12:26 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:26 INFO Configuration.deprecation: No unit for dfs.client.datanode-restart.timeout(30) assuming SECONDS
25/10/27 11:12:26 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm278
25/10/27 11:12:26 INFO yarn.Client: Requesting a new application from cluster with 6 NodeManagers
25/10/27 11:12:27 INFO conf.Configuration: resource-types.xml not found
25/10/27 11:12:27 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
25/10/27 11:12:27 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (65536 MB per container)
25/10/27 11:12:27 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
25/10/27 11:12:27 INFO yarn.Client: Setting up container launch context for our AM
25/10/27 11:12:27 INFO yarn.Client: Setting up the launch environment for our AM container
25/10/27 11:12:27 INFO yarn.Client: Preparing resources for our AM container
25/10/27 11:12:27 INFO yarn.Client: Uploading resource file:/tmp/spark-4f41f3d4-aa6e-4779-9d3c-acc11ab6eabe/__spark_conf__4677259297245588435.zip -> hdfs://Isilon-ET-hadoop-cdh.datalake01.eg:8020/user/cadmin/.sparkStaging/application_1756032282212_36893/__spark_conf__.zip
25/10/27 11:12:27 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:27 INFO Configuration.deprecation: No unit for dfs.client.datanode-restart.timeout(30) assuming SECONDS
25/10/27 11:12:27 INFO spark.SecurityManager: Changing view acls to: cadmin
25/10/27 11:12:27 INFO spark.SecurityManager: Changing modify acls to: cadmin
25/10/27 11:12:27 INFO spark.SecurityManager: Changing view acls groups to: 
25/10/27 11:12:27 INFO spark.SecurityManager: Changing modify acls groups to: 
25/10/27 11:12:27 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cadmin); groups with view permissions: Set(); users  with modify permissions: Set(cadmin); groups with modify permissions: Set()
25/10/27 11:12:27 INFO yarn.Client: Submitting application application_1756032282212_36893 to ResourceManager
25/10/27 11:12:27 INFO impl.YarnClientImpl: Submitted application application_1756032282212_36893
25/10/27 11:12:28 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:28 INFO yarn.Client: Application report for application_1756032282212_36893 (state: ACCEPTED)
25/10/27 11:12:28 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: root.users.cadmin
	 start time: 1761552747394
	 final status: UNDEFINED
	 tracking URL: http://lakemaster3.datalake01.eg:8088/proxy/application_1756032282212_36893/
	 user: cadmin
25/10/27 11:12:29 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:29 INFO yarn.Client: Application report for application_1756032282212_36893 (state: ACCEPTED)
25/10/27 11:12:29 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> lakemaster2.datalake01.eg,lakemaster3.datalake01.eg, PROXY_URI_BASES -> http://lakemaster2.datalake01.eg:8088/proxy/application_1756032282212_36893,http://lakemaster3.datalake01.eg:8088/proxy/application_1756032282212_36893, RM_HA_URLS -> lakemaster2.datalake01.eg:8088,lakemaster3.datalake01.eg:8088), /proxy/application_1756032282212_36893
25/10/27 11:12:29 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.
25/10/27 11:12:30 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:30 INFO yarn.Client: Application report for application_1756032282212_36893 (state: RUNNING)
25/10/27 11:12:30 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 10.208.244.26
	 ApplicationMaster RPC port: -1
	 queue: root.users.cadmin
	 start time: 1761552747394
	 final status: UNDEFINED
	 tracking URL: http://lakemaster3.datalake01.eg:8088/proxy/application_1756032282212_36893/
	 user: cadmin
25/10/27 11:12:30 INFO cluster.YarnClientSchedulerBackend: Application application_1756032282212_36893 has started running.
25/10/27 11:12:30 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1756032282212_36893 and attemptId None
25/10/27 11:12:30 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46248.
25/10/27 11:12:30 INFO netty.NettyBlockTransferService: Server created on lakenode3.datalake01.eg:46248
25/10/27 11:12:30 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/10/27 11:12:30 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, lakenode3.datalake01.eg, 46248, None)
25/10/27 11:12:30 INFO storage.BlockManagerMasterEndpoint: Registering block manager lakenode3.datalake01.eg:46248 with 4.1 GB RAM, BlockManagerId(driver, lakenode3.datalake01.eg, 46248, None)
25/10/27 11:12:30 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, lakenode3.datalake01.eg, 46248, None)
25/10/27 11:12:30 INFO storage.BlockManager: external shuffle service port = 7337
25/10/27 11:12:30 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, lakenode3.datalake01.eg, 46248, None)
25/10/27 11:12:30 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
25/10/27 11:12:30 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.
25/10/27 11:12:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3d2ebea9{/metrics/json,null,AVAILABLE,@Spark}
25/10/27 11:12:30 INFO scheduler.EventLoggingListener: Logging events to hdfs://Isilon-ET-hadoop-cdh.datalake01.eg:8020/user/spark/applicationHistory/application_1756032282212_36893
25/10/27 11:12:30 INFO util.Utils: Using initial executors = 10, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
25/10/27 11:12:30 INFO spark.SparkContext: Registered listener com.cloudera.spark.lineage.NavigatorAppListener
25/10/27 11:12:30 INFO logging.DriverLogger$DfsAsyncWriter: Started driver log file sync to: /user/spark/driverLogs/application_1756032282212_36893_driver.log
25/10/27 11:12:31 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:32 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:33 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:34 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:35 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:36 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:37 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:38 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:39 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:40 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:41 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:42 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:43 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:44 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:45 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:46 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:47 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:48 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:49 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:50 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:51 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:52 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:53 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:54 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:55 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:56 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:56 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
25/10/27 11:12:56 INFO internal.SharedState: loading hive config file: file:/etc/hive/conf.cloudera.hive/hive-site.xml
25/10/27 11:12:56 INFO internal.SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
25/10/27 11:12:56 INFO internal.SharedState: Warehouse path is '/user/hive/warehouse'.
25/10/27 11:12:56 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.
25/10/27 11:12:56 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@34ab1676{/SQL,null,AVAILABLE,@Spark}
25/10/27 11:12:56 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.
25/10/27 11:12:56 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a5d5f54{/SQL/json,null,AVAILABLE,@Spark}
25/10/27 11:12:56 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.
25/10/27 11:12:56 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@347996ed{/SQL/execution,null,AVAILABLE,@Spark}
25/10/27 11:12:56 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.
25/10/27 11:12:56 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ad98a08{/SQL/execution/json,null,AVAILABLE,@Spark}
25/10/27 11:12:56 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.
25/10/27 11:12:56 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@39b4af32{/static/sql,null,AVAILABLE,@Spark}
25/10/27 11:12:57 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
2025-10-27 11:12:57 INFO - Starting default short-term job for 2025-10-25 -> 2025-10-27
2025-10-27 11:12:57 INFO - Executing combined query on Spark SQL ...
2025-10-27 11:12:57 ERROR - Job failed: u"\nmismatched input '.' expecting <EOF>(line 11, pos 36)\n\n== SQL ==\n\n            SELECT\n                subscription_id,\n                app_name,\n                lower(app_category) AS app_category,\n                SUM(CAST(mbs AS DECIMAL(18,6))) AS total_mbs,\n                CAST(SUM(no_of_sessions) AS INT) AS total_sessions,\n                COUNT(*) AS access_days,\n                'app' AS type,\n                day\n            FROM hive.analytics_prod.online_interest_protocols_daily\n------------------------------------^^^\n            WHERE day BETWEEN '2025-10-25' AND '2025-10-27'\n            GROUP BY subscription_id, app_name, app_category, day\n\n            UNION ALL\n\n            SELECT\n                subscription_id,\n                app_name,\n                lower(app_category) AS app_category,\n                0 AS total_mbs,\n                SUM(no_of_sessions) AS total_sessions,\n                COUNT(*) AS access_days,\n                'url' AS type,\n                day\n            FROM hive.analytics_prod.online_interest_urls_daily\n            WHERE day BETWEEN '2025-10-25' AND '2025-10-27'\n            GROUP BY subscription_id, app_name, app_category, day\n\n            UNION ALL\n\n            SELECT\n                subscription_id,\n                app_name,\n                lower(app_category) AS app_category,\n                0 AS total_mbs,\n                SUM(no_of_sessions) AS total_sessions,\n                COUNT(*) AS access_days,\n                'sms' AS type,\n                day\n            FROM hive.analytics_prod.online_interest_sms_daily\n            WHERE day BETWEEN '2025-10-25' AND '2025-10-27'\n            GROUP BY subscription_id, app_name, app_category, day\n\n            UNION ALL\n\n            SELECT\n                subscription_id,\n                app_name,\n                lower(app_category) AS app_category,\n                0 AS total_mbs,\n                SUM(no_of_sessions) AS total_sessions,\n                COUNT(*) AS access_days,\n                'og' AS type,\n                day\n            FROM hive.analytics_prod.online_interest_og_calls_daily\n            WHERE day BETWEEN '2025-10-25' AND '2025-10-27'\n            GROUP BY subscription_id, app_name, app_category, day\n        \n"
25/10/27 11:12:57 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/27 11:12:57 INFO server.AbstractConnector: Stopped Spark@1e891e0d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
25/10/27 11:12:57 INFO ui.SparkUI: Stopped Spark web UI at http://lakenode3.datalake01.eg:4040
25/10/27 11:12:57 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread
25/10/27 11:12:57 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors
25/10/27 11:12:57 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
25/10/27 11:12:57 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
25/10/27 11:12:57 INFO cluster.YarnClientSchedulerBackend: Stopped
25/10/27 11:12:57 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/10/27 11:12:57 INFO memory.MemoryStore: MemoryStore cleared
25/10/27 11:12:57 INFO storage.BlockManager: BlockManager stopped
25/10/27 11:12:57 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
25/10/27 11:12:57 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/10/27 11:12:57 INFO spark.SparkContext: Successfully stopped SparkContext
Traceback (most recent call last):
  File "/home/cadmin/Scripts/Analytics_Scripts/oim_short_term/combined_query_daily.py", line 167, in <module>
    main()
  File "/home/cadmin/Scripts/Analytics_Scripts/oim_short_term/combined_query_daily.py", line 133, in main
    df_base = spark.sql(query)
  File "/opt/cloudera/parcels/CDH-6.3.3-1.cdh6.3.3.p0.1796617/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 778, in sql
  File "/opt/cloudera/parcels/CDH-6.3.3-1.cdh6.3.3.p0.1796617/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/opt/cloudera/parcels/CDH-6.3.3-1.cdh6.3.3.p0.1796617/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 73, in deco
pyspark.sql.utils.ParseException: u"\nmismatched input '.' expecting <EOF>(line 11, pos 36)\n\n== SQL ==\n\n            SELECT\n                subscription_id,\n                app_name,\n                lower(app_category) AS app_category,\n                SUM(CAST(mbs AS DECIMAL(18,6))) AS total_mbs,\n                CAST(SUM(no_of_sessions) AS INT) AS total_sessions,\n                COUNT(*) AS access_days,\n                'app' AS type,\n                day\n            FROM hive.analytics_prod.online_interest_protocols_daily\n------------------------------------^^^\n            WHERE day BETWEEN '2025-10-25' AND '2025-10-27'\n            GROUP BY subscription_id, app_name, app_category, day\n\n            UNION ALL\n\n            SELECT\n                subscription_id,\n                app_name,\n                lower(app_category) AS app_category,\n                0 AS total_mbs,\n                SUM(no_of_sessions) AS total_sessions,\n                COUNT(*) AS access_days,\n                'url' AS type,\n                day\n            FROM hive.analytics_prod.online_interest_urls_daily\n            WHERE day BETWEEN '2025-10-25' AND '2025-10-27'\n            GROUP BY subscription_id, app_name, app_category, day\n\n            UNION ALL\n\n            SELECT\n                subscription_id,\n                app_name,\n                lower(app_category) AS app_category,\n                0 AS total_mbs,\n                SUM(no_of_sessions) AS total_sessions,\n                COUNT(*) AS access_days,\n                'sms' AS type,\n                day\n            FROM hive.analytics_prod.online_interest_sms_daily\n            WHERE day BETWEEN '2025-10-25' AND '2025-10-27'\n            GROUP BY subscription_id, app_name, app_category, day\n\n            UNION ALL\n\n            SELECT\n                subscription_id,\n                app_name,\n                lower(app_category) AS app_category,\n                0 AS total_mbs,\n                SUM(no_of_sessions) AS total_sessions,\n                COUNT(*) AS access_days,\n                'og' AS type,\n                day\n            FROM hive.analytics_prod.online_interest_og_calls_daily\n            WHERE day BETWEEN '2025-10-25' AND '2025-10-27'\n            GROUP BY subscription_id, app_name, app_category, day\n        \n"
25/10/27 11:12:57 INFO util.ShutdownHookManager: Shutdown hook called
25/10/27 11:12:57 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-4f41f3d4-aa6e-4779-9d3c-acc11ab6eabe
25/10/27 11:12:57 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-9422eabb-0de9-4f13-96f9-5cb220a57221
25/10/27 11:12:57 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-4f41f3d4-aa6e-4779-9d3c-acc11ab6eabe/pyspark-76ecf964-78a9-46f0-baf9-bfbc91290b20
2025-10-27 11:12:58 - ERROR: Short-term OIM job failed. Check logs: /home/cadmin/Scripts/Analytics_Scripts/oim_short_term/logs/oim_short_term_2025-10-27.log
