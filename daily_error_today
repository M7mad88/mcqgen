25/10/28 11:12:52 INFO spark.SparkContext: Submitted application: Online_Model_Short_Term_Daily
25/10/28 11:12:52 INFO spark.SecurityManager: Changing view acls to: cadmin
25/10/28 11:12:52 INFO spark.SecurityManager: Changing modify acls to: cadmin
25/10/28 11:12:52 INFO spark.SecurityManager: Changing view acls groups to: 
25/10/28 11:12:52 INFO spark.SecurityManager: Changing modify acls groups to: 
25/10/28 11:12:52 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cadmin); groups with view permissions: Set(); users  with modify permissions: Set(cadmin); groups with modify permissions: Set()
25/10/28 11:12:53 INFO util.Utils: Successfully started service 'sparkDriver' on port 44945.
25/10/28 11:12:53 INFO spark.SparkEnv: Registering MapOutputTracker
25/10/28 11:12:53 INFO spark.SparkEnv: Registering BlockManagerMaster
25/10/28 11:12:53 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/10/28 11:12:53 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/10/28 11:12:53 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-44ac480d-f110-456c-89a1-7d825105d226
25/10/28 11:12:53 INFO memory.MemoryStore: MemoryStore started with capacity 3.0 GB
25/10/28 11:12:53 INFO spark.SparkEnv: Registering OutputCommitCoordinator
25/10/28 11:12:53 INFO util.log: Logging initialized @1693ms
25/10/28 11:12:53 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-09-04T23:11:46+02:00, git hash: 3ce520221d0240229c862b122d2b06c12a625732
25/10/28 11:12:53 INFO server.Server: Started @1758ms
25/10/28 11:12:53 INFO server.AbstractConnector: Started ServerConnector@8892de1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
25/10/28 11:12:53 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@e368659{/jobs,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30295751{/jobs/json,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9b891b9{/jobs/job,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5bf18db5{/jobs/job/json,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2198ca45{/stages,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c60b296{/stages/json,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@56f9d0dc{/stages/stage,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f82b155{/stages/stage/json,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1765ef49{/stages/pool,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d9b8cd{/stages/pool/json,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1902343f{/storage,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3902653{/storage/json,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3bb78ecf{/storage/rdd,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@736056a4{/storage/rdd/json,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7bcb373b{/environment,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@796b24af{/environment/json,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7ead1cbb{/executors,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f09b0bc{/executors/json,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d592c89{/executors/threadDump,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d373bd1{/executors/threadDump/json,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b69b608{/static,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23765db3{/,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@80f70af{/api,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53cf367c{/jobs/job/kill,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@711cd67e{/stages/stage/kill,null,AVAILABLE,@Spark}
25/10/28 11:12:53 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://lakenode3.datalake01.eg:4040
25/10/28 11:12:53 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/28 11:12:53 INFO Configuration.deprecation: No unit for dfs.client.datanode-restart.timeout(30) assuming SECONDS
25/10/28 11:12:53 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm278
25/10/28 11:12:54 INFO yarn.Client: Requesting a new application from cluster with 6 NodeManagers
25/10/28 11:12:54 INFO conf.Configuration: resource-types.xml not found
25/10/28 11:12:54 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
25/10/28 11:12:54 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (65536 MB per container)
25/10/28 11:12:54 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
25/10/28 11:12:54 INFO yarn.Client: Setting up container launch context for our AM
25/10/28 11:12:54 INFO yarn.Client: Setting up the launch environment for our AM container
25/10/28 11:12:54 INFO yarn.Client: Preparing resources for our AM container
25/10/28 11:12:54 INFO yarn.Client: Uploading resource file:/tmp/spark-5ddb35a8-17f0-46f6-87d1-db68c900cff7/__spark_conf__1347195798045042348.zip -> hdfs://Isilon-ET-hadoop-cdh.datalake01.eg:8020/user/cadmin/.sparkStaging/application_1756032282212_37370/__spark_conf__.zip
25/10/28 11:12:54 INFO Configuration.deprecation: No unit for dfs.client.datanode-restart.timeout(30) assuming SECONDS
25/10/28 11:12:54 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/28 11:12:54 INFO spark.SecurityManager: Changing view acls to: cadmin
25/10/28 11:12:54 INFO spark.SecurityManager: Changing modify acls to: cadmin
25/10/28 11:12:54 INFO spark.SecurityManager: Changing view acls groups to: 
25/10/28 11:12:54 INFO spark.SecurityManager: Changing modify acls groups to: 
25/10/28 11:12:54 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(cadmin); groups with view permissions: Set(); users  with modify permissions: Set(cadmin); groups with modify permissions: Set()
25/10/28 11:12:54 INFO yarn.Client: Submitting application application_1756032282212_37370 to ResourceManager
25/10/28 11:12:54 INFO impl.YarnClientImpl: Submitted application application_1756032282212_37370
25/10/28 11:12:55 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/28 11:12:55 INFO yarn.Client: Application report for application_1756032282212_37370 (state: ACCEPTED)
25/10/28 11:12:55 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: root.users.cadmin
	 start time: 1761639174414
	 final status: UNDEFINED
	 tracking URL: http://lakemaster3.datalake01.eg:8088/proxy/application_1756032282212_37370/
	 user: cadmin
25/10/28 11:12:56 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/28 11:12:56 INFO yarn.Client: Application report for application_1756032282212_37370 (state: ACCEPTED)
25/10/28 11:12:56 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> lakemaster2.datalake01.eg,lakemaster3.datalake01.eg, PROXY_URI_BASES -> http://lakemaster2.datalake01.eg:8088/proxy/application_1756032282212_37370,http://lakemaster3.datalake01.eg:8088/proxy/application_1756032282212_37370, RM_HA_URLS -> lakemaster2.datalake01.eg:8088,lakemaster3.datalake01.eg:8088), /proxy/application_1756032282212_37370
25/10/28 11:12:56 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.
25/10/28 11:12:57 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/28 11:12:57 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
25/10/28 11:12:57 INFO yarn.Client: Application report for application_1756032282212_37370 (state: RUNNING)
25/10/28 11:12:57 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 10.208.244.28
	 ApplicationMaster RPC port: -1
	 queue: root.users.cadmin
	 start time: 1761639174414
	 final status: UNDEFINED
	 tracking URL: http://lakemaster3.datalake01.eg:8088/proxy/application_1756032282212_37370/
	 user: cadmin
25/10/28 11:12:57 INFO cluster.YarnClientSchedulerBackend: Application application_1756032282212_37370 has started running.
25/10/28 11:12:57 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1756032282212_37370 and attemptId None
25/10/28 11:12:57 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46795.
25/10/28 11:12:57 INFO netty.NettyBlockTransferService: Server created on lakenode3.datalake01.eg:46795
25/10/28 11:12:57 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/10/28 11:12:57 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, lakenode3.datalake01.eg, 46795, None)
25/10/28 11:12:57 INFO storage.BlockManagerMasterEndpoint: Registering block manager lakenode3.datalake01.eg:46795 with 3.0 GB RAM, BlockManagerId(driver, lakenode3.datalake01.eg, 46795, None)
25/10/28 11:12:57 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, lakenode3.datalake01.eg, 46795, None)
25/10/28 11:12:57 INFO storage.BlockManager: external shuffle service port = 7337
25/10/28 11:12:57 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, lakenode3.datalake01.eg, 46795, None)
25/10/28 11:12:57 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.
25/10/28 11:12:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b4168ff{/metrics/json,null,AVAILABLE,@Spark}
25/10/28 11:12:57 INFO scheduler.EventLoggingListener: Logging events to hdfs://Isilon-ET-hadoop-cdh.datalake01.eg:8020/user/spark/applicationHistory/application_1756032282212_37370
25/10/28 11:12:57 INFO spark.SparkContext: Registered listener com.cloudera.spark.lineage.NavigatorAppListener
25/10/28 11:12:57 INFO logging.DriverLogger$DfsAsyncWriter: Started driver log file sync to: /user/spark/driverLogs/application_1756032282212_37370_driver.log
25/10/28 11:12:58 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/28 11:12:59 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all
25/10/28 11:13:00 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.208.244.28:57294) with ID 1
25/10/28 11:13:00 INFO storage.BlockManagerMasterEndpoint: Registering block manager lakenode5.datalake01.eg:41441 with 5.2 GB RAM, BlockManagerId(1, lakenode5.datalake01.eg, 41441, None)
25/10/28 11:13:23 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
25/10/28 11:13:23 INFO internal.SharedState: loading hive config file: file:/etc/hive/conf.cloudera.hive/hive-site.xml
25/10/28 11:13:23 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('/user/hive/warehouse') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
25/10/28 11:13:23 INFO internal.SharedState: Warehouse path is '/user/hive/warehouse'.
25/10/28 11:13:23 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.
25/10/28 11:13:23 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e983358{/SQL,null,AVAILABLE,@Spark}
25/10/28 11:13:23 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.
25/10/28 11:13:23 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33c5089{/SQL/json,null,AVAILABLE,@Spark}
25/10/28 11:13:23 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.
25/10/28 11:13:23 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6cdf0226{/SQL/execution,null,AVAILABLE,@Spark}
25/10/28 11:13:23 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.
25/10/28 11:13:23 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c4733b2{/SQL/execution/json,null,AVAILABLE,@Spark}
25/10/28 11:13:23 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.
25/10/28 11:13:23 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1352e3ef{/static/sql,null,AVAILABLE,@Spark}
25/10/28 11:13:24 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
2025-10-28 11:13:24 INFO - Starting default short-term run for 2025-10-26 to 2025-10-28
2025-10-28 11:13:24 INFO - Executing base query for short-term run...
25/10/28 11:13:24 INFO conf.HiveConf: Found configuration file file:/etc/hive/conf.cloudera.hive/hive-site.xml
25/10/28 11:13:24 INFO hive.HiveUtils: Initializing HiveMetastoreConnection version 2.1 using Spark classes.
25/10/28 11:13:24 INFO conf.HiveConf: Found configuration file file:/etc/hive/conf.cloudera.hive/hive-site.xml
25/10/28 11:13:24 INFO session.SessionState: Created HDFS directory: /tmp/hive/cadmin/ccb97048-e03f-44fa-b130-7e44f4022476
25/10/28 11:13:24 INFO session.SessionState: Created local directory: /tmp/cadmin/ccb97048-e03f-44fa-b130-7e44f4022476
25/10/28 11:13:24 INFO session.SessionState: Created HDFS directory: /tmp/hive/cadmin/ccb97048-e03f-44fa-b130-7e44f4022476/_tmp_space.db
25/10/28 11:13:24 INFO client.HiveClientImpl: Warehouse location for Hive client (version 2.1.1) is /user/hive/warehouse
25/10/28 11:13:25 INFO hive.metastore: HMS client filtering is enabled.
25/10/28 11:13:25 INFO hive.metastore: Trying to connect to metastore with URI thrift://lakemaster1.datalake01.eg:9083
25/10/28 11:13:25 INFO hive.metastore: Opened a connection to metastore, current connections: 1
25/10/28 11:13:25 INFO hive.metastore: Connected to metastore.
25/10/28 11:13:25 INFO metadata.Hive: Registering function fix_arabic FixArabicUDF
2025-10-28 11:13:26 INFO - Writing to target table analytics_prod.oim_short_term_raw for dt=2025-10-28...
2025-10-28 11:13:27 ERROR - Failed to write to target table: u'`analytics_prod`.`oim_short_term_raw` requires that the data to be inserted have the same number of columns as the target table: target table has 9 column(s) but the inserted data has 8 column(s), including 1 partition column(s) having constant value(s).;'
25/10/28 11:13:27 INFO server.AbstractConnector: Stopped Spark@8892de1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
25/10/28 11:13:27 INFO ui.SparkUI: Stopped Spark web UI at http://lakenode3.datalake01.eg:4040
25/10/28 11:13:27 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread
25/10/28 11:13:27 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors
25/10/28 11:13:27 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
25/10/28 11:13:27 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
25/10/28 11:13:27 INFO cluster.YarnClientSchedulerBackend: Stopped
25/10/28 11:13:27 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/10/28 11:13:27 INFO memory.MemoryStore: MemoryStore cleared
25/10/28 11:13:27 INFO storage.BlockManager: BlockManager stopped
25/10/28 11:13:27 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
25/10/28 11:13:27 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/10/28 11:13:27 INFO spark.SparkContext: Successfully stopped SparkContext
25/10/28 11:13:27 INFO util.ShutdownHookManager: Shutdown hook called
25/10/28 11:13:27 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-0a1d58da-a0f0-46c3-a4c6-c6f8d96589ad
25/10/28 11:13:27 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-5ddb35a8-17f0-46f6-87d1-db68c900cff7
25/10/28 11:13:27 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-5ddb35a8-17f0-46f6-87d1-db68c900cff7/pyspark-a9504bb3-3a8d-48f3-b133-084637deba77
2025-10-28 11:13:27 - ERROR: Short-term job failed. Check logs for details.
